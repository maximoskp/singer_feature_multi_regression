{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask_dataset import SingerMultiTaskDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from models import HuBERTMultiHead\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_folder = '/media/maximos/9C33-6BBD/data/melos_singers/Rebetika_vowels/train/'\n",
    "csv_path = '/media/maximos/9C33-6BBD/data/melos_singers/features/multitask_targets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'names', 'singer_id', 'Pitch', 'SpectralCentroid', 'SpectralSpread', 'SpectralSkewness', 'SpectralKurtosis', 'SpectralFlatness', 'SpectralCrest', 'SpectralSlope', 'SpectralDecrease', 'SpectralRollOff', 'SpectralVariation', 'SpectralFlux', 'HarmonicSpectralDeviation', 'Tristimulus_1', 'Tristimulus_2', 'Tristimulus_3', 'HarmonicOddToEvenRatio', 'Inharmonicity', 'HarmonicEnergy', 'NoiseEnergy', 'Noisiness', 'HarmonicToNoiseEnergy', 'PartialsToNoiseEnergy', 'F1_Hz', 'F2_Hz', 'F3_HZ', 'F4_Hz', 'Rate', 'Depth', 'Regularity']\n",
      "['singer_id', 'Pitch', 'SpectralCentroid', 'SpectralSpread', 'SpectralSkewness', 'SpectralKurtosis', 'SpectralFlatness', 'SpectralCrest', 'SpectralSlope', 'SpectralDecrease', 'SpectralRollOff', 'SpectralVariation', 'SpectralFlux', 'HarmonicSpectralDeviation', 'Tristimulus_1', 'Tristimulus_2', 'Tristimulus_3', 'HarmonicOddToEvenRatio', 'Inharmonicity', 'HarmonicEnergy', 'NoiseEnergy', 'Noisiness', 'HarmonicToNoiseEnergy', 'PartialsToNoiseEnergy', 'F1_Hz', 'F2_Hz', 'F3_HZ', 'F4_Hz', 'Rate', 'Depth', 'Regularity']\n"
     ]
    }
   ],
   "source": [
    "feats = pd.read_csv(csv_path, delimiter=',')\n",
    "features_list = list(feats.columns)\n",
    "print(features_list)\n",
    "del(features_list[:2])\n",
    "print(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pitch': 1, 'SpectralCentroid': 1, 'SpectralSpread': 1, 'SpectralSkewness': 1, 'SpectralKurtosis': 1, 'SpectralFlatness': 1, 'SpectralCrest': 1, 'SpectralSlope': 1, 'SpectralDecrease': 1, 'SpectralRollOff': 1, 'SpectralVariation': 1, 'SpectralFlux': 1, 'HarmonicSpectralDeviation': 1, 'Tristimulus_1': 1, 'Tristimulus_2': 1, 'Tristimulus_3': 1, 'HarmonicOddToEvenRatio': 1, 'Inharmonicity': 1, 'HarmonicEnergy': 1, 'NoiseEnergy': 1, 'Noisiness': 1, 'HarmonicToNoiseEnergy': 1, 'PartialsToNoiseEnergy': 1, 'F1_Hz': 1, 'F2_Hz': 1, 'F3_HZ': 1, 'F4_Hz': 1, 'singer_id': 6}\n"
     ]
    }
   ],
   "source": [
    "task_labels_num_out = {}\n",
    "for i in range(1, len(features_list)-3, 1):\n",
    "    task_labels_num_out[features_list[i]] = 1\n",
    "# add singer identification\n",
    "task_labels_num_out['singer_id'] = feats['singer_id'].max()+1 # accounting for zero\n",
    "print(task_labels_num_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type hubert to instantiate a model of type wav2vec2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = HuBERTMultiHead(task_labels_num_out=task_labels_num_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SingerMultiTaskDataset(train_audio_folder, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': array([ 9.6231786e-05, -4.8065151e-04, -1.0562916e-03, ...,\n",
      "        2.1723036e-02,  3.0537494e-02,  3.0633982e-02], dtype=float32), 'labels': {'Unnamed: 0': 235, 'names': 'keti_grei_e_7', 'singer_id': 3, 'Pitch': 442.967567138073, 'SpectralCentroid': 1649.18342861135, 'SpectralSpread': 1091.55349106278, 'SpectralSkewness': 1.37180571109231, 'SpectralKurtosis': 5.05492277926304, 'SpectralFlatness': 1.19733652424461e-09, 'SpectralCrest': 0.246353359792086, 'SpectralSlope': 1.0414362293082e-06, 'SpectralDecrease': 0.0087732118944919, 'SpectralRollOff': 2131.34777604787, 'SpectralVariation': 0.861851957216352, 'SpectralFlux': 0.001313200733224, 'HarmonicSpectralDeviation': 0.012296689893735, 'Tristimulus_1': 0.250974023096942, 'Tristimulus_2': 0.590761277109722, 'Tristimulus_3': 0.140094494653077, 'HarmonicOddToEvenRatio': 0.381924585198844, 'Inharmonicity': 0.0848189851447228, 'HarmonicEnergy': 0.0184961146711919, 'NoiseEnergy': 0.029466693442144, 'Noisiness': 0.498319194751083, 'HarmonicToNoiseEnergy': 0.941382100869722, 'PartialsToNoiseEnergy': 1.00675225464119, 'F1_Hz': 776.231683, 'F2_Hz': 1771.822117, 'F3_HZ': 3258.535735, 'F4_Hz': 3876.085193, 'Rate': 0.97878196, 'Depth': 0.109985736, 'Regularity': 0.470908039}}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_data, batch_size=8, shuffle=True, collate_fn=model.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': <multitask_dataset.SingerMultiTaskDataset object at 0x7c48c01b5ee0>, 'num_workers': 0, 'prefetch_factor': None, 'pin_memory': False, 'pin_memory_device': '', 'timeout': 0, 'worker_init_fn': None, '_DataLoader__multiprocessing_context': None, '_dataset_kind': 0, 'batch_size': 8, 'drop_last': False, 'sampler': <torch.utils.data.sampler.RandomSampler object at 0x7c47524327b0>, 'batch_sampler': <torch.utils.data.sampler.BatchSampler object at 0x7c4751b92de0>, 'generator': None, 'collate_fn': <bound method HuBERTMultiHead.collate_fn of HuBERTMultiHead(\n",
      "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Wav2Vec2GroupNormConvLayer(\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_projection): Wav2Vec2FeatureProjection(\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): Wav2Vec2Encoder(\n",
      "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "      (conv): ParametrizedConv1d(\n",
      "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "        (parametrizations): ModuleDict(\n",
      "          (weight): ParametrizationList(\n",
      "            (0): _WeightNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (padding): Wav2Vec2SamePadLayer()\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
      "        (attention): Wav2Vec2Attention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Wav2Vec2FeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hubert): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2GroupNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2Encoder(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>, 'persistent_workers': False, '_DataLoader__initialized': True, '_IterableDataset_len_called': None, '_iterator': None}\n"
     ]
    }
   ],
   "source": [
    "print(dataloader.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "33\n",
      "{'Unnamed: 0': [174, 230, 204, 7, 272, 225, 91, 259], 'names': ['kazantzidis_old_w_5', 'keti_grei_e_2', 'kazantzidis_w_4', 'bellou_a_8', 'ninou_a_8', 'keti_grei_a_8', 'kazantzidis_a_9', 'keti_grei_w_4'], 'singer_id': [2, 3, 2, 0, 4, 3, 2, 3], 'Pitch': [169.509689056344, 386.964460421979, 197.661844144218, 395.437895227828, 323.522932600798, 262.327260849853, 278.086634270947, 461.378767764507], 'SpectralCentroid': [741.608425632974, 1944.46030522022, 920.491630236247, 1537.10933910004, 1790.93267715259, 1530.14928974613, 1618.23388504433, 1571.91868251572], 'SpectralSpread': [655.809039800715, 1234.60489570977, 707.152920947376, 874.147105922819, 1292.50766099663, 939.844720532441, 1164.1024347626, 1057.52332222411], 'SpectralSkewness': [2.71044148741626, 0.4476224705103, 2.03307385493606, 1.63441348519071, 0.664486877911191, 1.08909937109089, 1.2920718861889, 1.28041427433689], 'SpectralKurtosis': [10.4068383929333, 2.08251175892443, 6.93464546720047, 6.02349663543939, 2.10554461675713, 3.83876519875982, 3.96971406034843, 3.91666525569264], 'SpectralFlatness': [6.89220508813804e-05, 1.04005723405062e-05, 1.94693179924629e-08, 3.46988983123124e-07, 0.0055180295409843, 2.23345363951747e-07, 3.30614728008953e-06, 6.58471270203532e-07], 'SpectralCrest': [0.421773441314854, 0.220654497659052, 0.3185747422645, 0.236644312696034, 0.118170627820423, 0.218500277170955, 0.192482488596572, 0.227177122326633], 'SpectralSlope': [-5.02863455873382e-06, 3.70959244819385e-07, 1.60366527368881e-06, 4.64126558865793e-07, -6.58549093213072e-07, 8.85691436046296e-07, 1.72873872518816e-07, 4.96960344907828e-07], 'SpectralDecrease': [0.0888409532642442, 0.0247730885262103, 0.028221521363198, 0.0234691196399361, 0.02494255060133, 0.014593599702303, 0.0179939068925515, 0.0178336764727631], 'SpectralRollOff': [2538.52530945203, 4454.61545077137, 2398.33339388595, 3633.13001765702, 4260.38905239982, 3553.0779117794, 3348.15712508397, 2813.07911015761], 'SpectralVariation': [0.369878954303462, 0.75725464414276, 0.576370583856526, 0.709727657762483, 0.750360796603371, 0.71277312780358, 0.780561948631665, 0.700207925234161], 'SpectralFlux': [0.0061484275020128, 0.0018780338623283, 0.0028863339066239, 0.0018745158526005, 0.001211045312918, 0.0012430734731557, 0.0019562466327443, 0.0027296883007788], 'HarmonicSpectralDeviation': [0.0110721520150845, 0.010380171330876, 0.0096251016697772, 0.0053593465010438, 0.0029118532891883, 0.0047917901375114, 0.0040108567761625, 0.0101648172695786], 'Tristimulus_1': [0.120842972369367, 0.188702887550432, 0.0610796704732602, 0.125523811971624, 0.317055484889711, 0.0891278945618204, 0.20416590921318, 0.21912564133755], 'Tristimulus_2': [0.655913192332074, 0.386550376903236, 0.617983483427842, 0.769554810478148, 0.440549552690739, 0.499134638417091, 0.4405471580809, 0.693471793902597], 'Tristimulus_3': [0.21774586843912, 0.417971563649936, 0.303910912333446, 0.0956719666669321, 0.210641118178115, 0.331381291449875, 0.284878653218514, 0.0958515575012094], 'HarmonicOddToEvenRatio': [6.51619981884164, 0.780344768155664, 2.9190179594147, 0.45885616891038, 3.08579863932297, 3.09858556155448, 5.05787254501356, 0.669545569680682], 'Inharmonicity': [0.119677718113765, 0.0835440040620611, 0.113746614823281, 0.130978731763452, 0.181164609324335, 0.189587299160307, 0.236353017199376, 0.141353321635964], 'HarmonicEnergy': [0.0414747972363065, 0.0168393000399724, 0.029137338129208, 0.0068392221238724, 0.0079810023543486, 0.0054974160881239, 0.0078142015053684, 0.0364565953531288], 'NoiseEnergy': [0.0321033210806774, 0.0196552960307325, 0.024373255825434, 0.0158779447712371, 0.0179407164706847, 0.0214703888183178, 0.0392606770825585, 0.0627935632204153], 'Noisiness': [0.423750874903513, 0.483077687224855, 0.43831904523242, 0.534923006119379, 0.562199358019447, 0.532634031504696, 0.58452606809377, 0.526515314036427], 'HarmonicToNoiseEnergy': [1.35245511400705, 0.945112580730313, 1.22550298219757, 0.447248960807956, 0.481334738614647, 0.481361275201652, 0.221870616198528, 0.652777981198133], 'PartialsToNoiseEnergy': [1.35987713353441, 1.07017269636614, 1.28144386781473, 0.869441652682858, 0.778728479860801, 0.877474834099428, 0.710789056846628, 0.899280611587994], 'F1_Hz': [525.437715, 760.669282, 605.178373, 990.369851, 828.635679, 846.542696, 893.440904, 930.045419], 'F2_Hz': [1240.262965, 1918.535466, 1144.359086, 1600.849512, 1468.275999, 1651.49881, 1472.111582, 1414.08222], 'F3_HZ': [2559.25186, 2978.171533, 2284.427653, 3310.289025, 3177.093997, 2908.034632, 2727.731858, 3119.694695], 'F4_Hz': [3370.847723, 3604.167712, 3380.750318, 3974.473724, 3820.070955, 3975.14174, 3781.492949, 3686.481715], 'Rate': [1.384771905, 3.744904891, 1.050400152, 1.459878178, 5.981445313, 1.013327206, 2.970096983, 1.72265625], 'Depth': [0.031446561, 0.05775321, 0.061393318, 0.042124536, 0.054971912, 0.314520931, 0.126428972, 0.125342166], 'Regularity': [0.308651861, 0.260408294, 0.255179218, 0.128829065, 0.00187368, 0.086099327, 0.100767944, 0.323687579]}\n"
     ]
    }
   ],
   "source": [
    "print(len(b[0]['input_values']))\n",
    "print(len(b[0]['attention_mask']))\n",
    "print(len(b[1]))\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression 0\n",
      "regression 1\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "regression 0\n",
      "regression 2\n",
      "single_label_classification\n",
      "single_label_classification 0\n",
      "single_label_classification 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_normalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/maximos/9C33-6BBD/repos/singer_feature_multi_regression/models.py:115\u001b[0m, in \u001b[0;36mHuBERTMultiHead.forward\u001b[0;34m(self, audio_normalized, attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle_label_classification 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(\n\u001b[0;32m--> 115\u001b[0m             logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels[task_name]), \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "y = model(\n",
    "    audio_normalized=b[0]['input_values'],\n",
    "    attention_mask=b[0]['attention_mask'],\n",
    "    labels=b[1],\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/utils/generic.py:401\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    400\u001b[0m     inner_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "print(y['loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
